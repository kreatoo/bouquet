apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: litellm
data:
  config.yaml: |
    model_list:
      # DeepSeek R1 (MAI-DS-R1)
      - model_name: deepseek-r1
        litellm_params:
          model: github/MAI-DS-R1
      - model_name: deepseek-r1
        litellm_params:
          model: openrouter/microsoft/mai-ds-r1:free
      - model_name: deepseek-r1
        litellm_params:
          model: openai/microsoft/MAI-DS-R1-FP8
          api_base: https://llm.chutes.ai/v1
          api_key: "os.environ/CHUTES_API_KEY"

      # DeepSeek V3 0324
      - model_name: deepseek-v3-0324
        litellm_params:
          model: github/DeepSeek-V3-0324

      - model_name: deepseek-v3-0324
        litellm_params:
          model: openrouter/deepseek/deepseek-chat-v3-0324:free

      - model_name: deepseek-v3-0324
        litellm_params:
          model: openai/deepseek-ai/DeepSeek-V3-0324
          api_base: https://llm.chutes.ai/v1
          api_key: "os.environ/CHUTES_API_KEY"

      # Gemini 2.5 Pro 0325
      - model_name: gemini-2.5-pro-03-25
        litellm_params:
          model: openrouter/google/gemini-2.5-pro-exp-03-25:free
      - model_name: gemini-2.5-pro-03-25
        litellm_params:
          model: gemini/gemini-2.5-pro-exp-03-25

      # TODO: Remove this when Copilot is supported on LiteLLM
      - model_name: gemini-2.5-pro-03-25
        litellm_params:
          model: openai/gemini-2.5-pro-preview-03-25
          api_base: http://copilot-api.open-webui.svc.cluster.local:4141
          api_key: blabladoesntmatter

      # LLama 4 Maverick
      - model_name: llama-4-maverick
        litellm_params:
          model: openai/chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8
          api_base: https://llm.chutes.ai/v1
          api_key: "os.environ/CHUTES_API_KEY"
      - model_name: llama-4-maverick
        litellm_params:
          model: github/Llama-4-Maverick-17B-128E-Instruct-FP8
      - model_name: llama-4-maverick
        litellm_params:
          model: openrouter/meta-llama/llama-4-maverick:free

      # LLama 4 Scout
      - model_name: llama-4-scout
        litellm_params:
          model: github/Llama-4-Scout-17B-16E-Instruct
      - model_name: llama-4-scout
        litellm_params:
          model: openai/chutesai/Llama-4-Scout-17B-16E-Instruct
          api_base: https://llm.chutes.ai/v1
          api_key: "os.environ/CHUTES_API_KEY"
      - model_name: llama-4-scout
        litellm_params:
          model: openrouter/meta-llama/llama-4-scout:free

    litellm_settings:
      drop_params: True
      cache: true
      cache_params:
        type: redis
        host: dragonfly-litellm

    general_settings:
      alerting: ["slack"]
      alert_types:
        - "llm_exceptions"
        - "llm_too_slow"
        - "llm_requests_hanging"
        - "db_exceptions"
        - "daily_reports"
        - "new_model_added"
        - "outage_alerts"
