apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: litellm
data:
  config.yaml: "model_list:\n  # DeepSeek R1\n  - model_name: deepseek-r1\n    litellm_params:\n      model: github/DeepSeek-R1\n  - model_name: deepseek-r1\n    litellm_params:\n      model: openrouter/deepseek/deepseek-r1:free\n  - model_name: deepseek-r1\n    litellm_params:\n      model: openai/deepseek-ai/DeepSeek-R1\n      api_base: https://llm.chutes.ai/v1\n      api_key: \"os.environ/CHUTES_API_KEY\"\n  \n  # DeepSeek V3 0324\n  - model_name: deepseek-v3-0324\n    litellm_params:\n      model: github/DeepSeek-V3-0324\n  \n  - model_name: deepseek-v3-0324\n    litellm_params:\n      model: openrouter/deepseek/deepseek-chat-v3-0324:free\n  \n  - model_name: deepseek-v3-0324\n    litellm_params:\n      model: openai/deepseek-ai/DeepSeek-V3-0324\n      api_base: https://llm.chutes.ai/v1\n      api_key: \"os.environ/CHUTES_API_KEY\"\n\n  # Gemini 2.5 Pro 0325\n  - model_name: gemini-2.5-pro-03-25\n    litellm_params:\n      model: openrouter/google/gemini-2.5-pro-exp-03-25:free\n  - model_name: gemini-2.5-pro-03-25\n    litellm_params:\n      model: gemini/gemini-2.5-pro-exp-03-25\n  \n  # TODO: Remove this when Copilot is supported on LiteLLM\n  - model_name: gemini-2.5-pro-03-25\n    litellm_params:\n      model: openai/gemini-2.5-pro-preview-03-25\n      api_base: http://copilot-api.open-webui.svc.cluster.local:4141\n      api_key: blabladoesntmatter\n\n  # LLama 4 Maverick\n  - model_name: llama-4-maverick\n    litellm_params:\n      model: openai/chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8\n      api_base: https://llm.chutes.ai/v1\n      api_key: \"os.environ/CHUTES_API_KEY\"\n  - model_name: llama-4-maverick\n    litellm_params:\n      model: github/Llama-4-Maverick-17B-128E-Instruct-FP8\n  - model_name: llama-4-maverick\n    litellm_params:\n      model: openrouter/meta-llama/llama-4-maverick:free\n  \n  # LLama 4 Scout\n  - model_name: llama-4-scout\n    litellm_params:\n      model: github/Llama-4-Scout-17B-16E-Instruct\n  - model_name: llama-4-scout\n    litellm_params:\n      model: openai/chutesai/Llama-4-Scout-17B-16E-Instruct\n      api_base: https://llm.chutes.ai/v1\n      api_key: \"os.environ/CHUTES_API_KEY\"\n  - model_name: llama-4-scout\n    litellm_params:\n      model: openrouter/meta-llama/llama-4-scout:free\n\nlitellm_settings:\n  drop_params: True\n  cache: true\n  cache_params:\n    type: redis\n    host: dragonfly-litellm\n\ngeneral_settings:\n  alerting: [\"slack\"]\n  alert_types:\n    - \"llm_exceptions\"\n    - \"llm_too_slow\"\n    - \"llm_requests_hanging\"\n    - \"db_exceptions\"\n    - \"daily_reports\"\n    - \"new_model_added\"\n    - \"outage_alerts\"\n"
